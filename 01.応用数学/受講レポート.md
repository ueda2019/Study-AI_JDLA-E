
# chap01 線形代数


## sec01

### 行列の用途

- ベクトルの変換！
- 線形性を保ったまま、ベクトルを変換できる。

### 固有値・固有ベクトル

```
行列A ＊ ベクトルx = スカラーλ  ＊ ベクトルx
```

- 行列Aに対する、、、
    - 固有値 ＝＞ λ
    - 固有ベクトル ＝＞ x
- 利点
    - 行列と、行列やベクトルのかけ算は、面倒
    - スカラーとベクトルのかけ算になったら楽になる
- 留意点
    - どんな時でも成り立つわけではなく、
    - そんな風になる、特殊なxがある、ということ

### 特殊な行列

- 単位行列
    - かけても元の行列の値が変わらない
    - 対角線上が１で、それ以外が０
- 逆行列
    - かけると単位行列になる

### 行列式

- 正方行列の「おおきさ」みたいなもの
- 対角線をかけたもの同士の差（スカラになる）
    - 行列内のベクトルで囲まれた平行四辺形の面積
    - （マイナスになることもあるので「みたいなもの」）


## sec02


### 固有値分解

```
正方行列 A
固有値を対角線上に並べた行列 Λ
固有ベクトルを並べた行列 V
```
とすると、 
```
AV = VΛ
```
と関係づけられる。（右辺は、ベクトルの各要素に、固有値を一斉にかけてあげたいので、この順番の掛け算になる）

したがって、以下のように変形できる
```
A = V * Λ  * (Ｖ逆行列) 
```
これを固有値分解という


### 固有値分解の構成要素

```
元のA ＝ 固有ベクトル（を並べた）行列 * 固有値の行列 * 固有ベクトル行列の逆行列 
```

- 両脇は、固有ベクトルは何かの定数倍、と複数になり、それほど重要じゃない。
- 大事なことは「固有値の行列」を導出できたこと！

### 固有値分解の意義

固有値に、元行列の特徴が現れる

- ものすごく小さければ無視していいデータかもしれない
    - データを減らせる、など
- ここが似ていれば、同じようなデータかもしれない
    - 同じ音声データを見つける、など


## sec03

### 特異値分解

特殊な単位ベクトルがある場合、正方行列以外でも似たことはできる。

特殊な、とは、、、、
```
行列M * vベクトル = σスカラ * uベクトル
```
vとuは要素の数も違ってしまっているので、あまり意味がないように見えるが、

```
行列Mの転置行列  * uベクトル = σスカラ * vベクトル
```
 と、そのuに、転置Mをかけたら、やっと固有値の右辺になるような場合、特異値分解できる

```
M ＝ U * S(シグマ) * Vの逆行列
```

u/vは単位ベクトル（大きさ１のベクトル）とする。本当は何かの定数倍、でもよいが、計算をシンプルにするため


### 特異値の求め方


```
M * V = U * S
M = U * S * Vの逆行列

```

転置の方は、


```
Ｍ転置 * U = V * S転置（Ｖとうまくくっつけるために転置）
Ｍ転置 = V * S転置 * Ｕ逆
```

で、これらの積、

```
Ｍ * Ｍ転置 = ... = U * S * S転置 * U逆
```

S * S転置 のところは、要するにＳ（固有値）の二乗である。よって、このルートをとってあげれば、もともと単位のＭの固有値になる。Ｕは「左特異ベクトル」という


### 特異値と固有値

- 実は、順番的には特異値分解が最初。
- 特異値分解を、無理やり、つまり「Ｍ * Ｍ転置」して正方形行列にしたのが、固有値分解。


### 特異値分解の意義

- 基本、特異値も、固有値と同じ。
- 要素数が多いときなど、特異値出せば、ほとんどの要素が０になる。
- つまり「特徴が出る」ということ。



# chap02 確率・統計

## sec01

### 確率の種類

- 頻度確率
    - 客観確率
    - 発生する頻度
- ベイズ確率
    - 主観確率
    - 信念の度合い
    - ２つを合体させたり、など計算の対象となりうる

### 独立事象の同時確率

各確率のシンプルな掛け算になる

```
P( X = x, Y = y )
= P( X = x) *  P( Y = y )
= P( Y = y , X = x )
```

### 条件付き確率

ある事象X=ｘが与えられたもとで、Y=yとなる確率

```
P( Y = y | X = x )
= XとY(独立事象)が同時に起きる確率 / Xが起きる確率
```
- 例としては、「雨降っているときに、事故る確率」など。
- ※語順に気を付ける、ＩＦが後ろ。


## sec02

### ベイズ則

```
P( X = x | Y = y )  P( Y = y ) 
= P( Y = y | X = x )  P( X = x ) 
```

- 主観と論理が両立している
- 機械学習的にも、条件の入れ替えなどの操作はあるので、便利

### 確率変数

- 事象と結びつけられた数値、事象そのもの
    - 事象のラベル、つまり、名前の場合もあるし、
    - 事象そのもの、の時もある

```
１：1万円もらえる
２：2万円もらえる
....
```

### 確率分布

- 事象の発生する確率の分布
    - 離散値なら表化可能


### 期待値

- その分布における、確率変数の、「平均値」or「ありえそうな値」
    - 確率変数がギャンブルなら、いくらくらい儲かるのか（損するのか）
    - 1万 × 発生確率 + 2万 × 発生確率 ....
- 離散はシグマで足して、連続値はインテグラで積分で。


## sec03

### 分散の計算

- データの散らばり具合
    - それぞれの要素が、期待値からどれだけズレているか、を2乗（マイナス対策）
    - それの平均（E => 平均）

```
Var(f) = E( ( f(X=x) - E(f) )**2 )
```

### 共分散

- ２つのデータ系列の傾向の違い
    - (「年収」「貯蓄額」の相関はあるか、など)
    - 正なら似てる傾向
    - 負なら逆の傾向
    - ゼロは関連性がない

```
Cov(f,g)= E( ( f(X=x) - E(f) )( g(Y=y) - E(g) ) )
```

### 分散の意義

- 平均(E)では、数値のばらつき具合がわからない。
- 期待値を出しても、その期待値からどれくらいズレてしまうのか、傾向がわからない。


## sec04

確率分布の種類（復習、確率分布＝事象の発生する確率の分布）

### ベルヌーイ分布

- コイントスのイメージ（0 or 1, A or not A）
- だけど、裏表割合が等しくなくてもうまく扱える

```
P(x=x) = (μ ** x)  *  ( ( 1 - μ ) ** (1-x) )
```
- xは0か1なので、どちらかが０乗になって、実質消えて式一つで表せる。

### マルチヌーイ（カテゴリカル）分布

- さいころのイメージ
- でる割合が等しくなくてもうまく扱える

### 二項分布

- ベルヌーイ分布の多試行版

### ガウス分布

- 釣り鐘型の連続分布
- y = e ** -(x**2) を微調整したもの
    - ※ exp(a) = e(ネイピア) ** a


# chap03 情報理論

## sec01

### 自己情報量

```
I(x) = log(P(x)) = log(w(x))
```

- 底が2の時、単位はbit
- 底がeの時、単位はnat
- 大きければ大きいほど、珍しい
- ビットの個数を考えてみるとわかる
    - 2回に1回なら、1ビットでいいし、
    - 8回に1回なら、3ビット必要、大きい
    - で、2 ** 3だから、logにすると、わかりやすい
- 複数の確率を足し合わせで扱いたい
    - 掛け算だと、減ってしまうので


- ＜数学的復習＞
    - b ** -aとは
        - マイナス乗とは、
        - もう一回Ａをかけるのと逆なので、
        - もう一回Ａで割れ、と言っている
    - logとは
        - 肩の上の数を教えてくれ、ということ

### （人間の）情報の捉え方

```
ΔＷ / Ｗ = ΔＩ
```

- 元々の量に対して、どれくらい増えたのか、を認識している
    - 同じ１つ増やすにしても、
    - 10個から11個は微妙
    - 1個から2個ならかなり大きい


### シャノンエントロピー

- 自己情報量の期待値（平均値）

```
H(x) = E(I(x))
```

## sec02

### KLダイバージェンス

```
KL ＝＞カルバック・ライブラー
ダイバージェンス divergence ＝＞発散
```

- 同じ事象・確率変数における、
- 異なる確率分布P、Qの違いを表す
    - 同じ事象で、違う確率とは？
        - 例えば、
        - もともとＱだったのが、新発見によってＰになった、とすると、
        - この分布の違いを観たい（どちらかというとＰを観たい）


### 交差エントロピー

- KLダイバージェンスの一部を取り出したもの
    - Qについての自己情報量をPの分布で平均
        - これを「交差」と表現
- 用途
    - とにかく珍しいモノを抽出して扱いたい
    - 例えば、通信量が限られていたら、異常状態のみを送付したい、など

